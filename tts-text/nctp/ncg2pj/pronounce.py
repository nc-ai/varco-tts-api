import re
import logging

from packaging.version import parse as V
from typing import Iterable, List, Optional, Union

import inflect
import string
from typing import Union
from copy import copy
# from nctp.japanese import JPN_SYMBOLS
# from nctp.symbols import SYMBOLS, SPACE
# from nctp.common import strip_diacritics
import pyopenjtalk
import pykakasi
import difflib


SPECIAL_NOTES = 'ã€‚ã€ï¼Ÿï¼.;ï¼›:,ï¼Œ:?!~-ğŸ˜¦ğŸ˜§ğŸ˜ğŸ˜‘ğŸ˜ƒğŸ˜„ğŸ˜”ğŸ˜ğŸ˜«ğŸ˜±ğŸ˜¤ğŸ˜¬ğŸ™ˆğŸ™‰ğŸ˜­ğŸ˜¢ğŸ¤ğŸ¢ğŸŠğŸ‹ '
SPECIAL_NOTES_PATTERN = r"[ã€‚ã€ï¼Ÿï¼.;ï¼šï¼›:,ï¼Œ:?!~\-ğŸ˜¦ğŸ˜§ğŸ˜ğŸ˜‘ğŸ˜ƒğŸ˜„ğŸ˜”ğŸ˜ğŸ˜«ğŸ˜±ğŸ˜¤ğŸ˜¬ğŸ™ˆğŸ™‰ğŸ˜­ğŸ˜¢ğŸ¤ğŸ¢ğŸŠğŸ‹ ]"

x2_consonant = {"sh": "S", "ry": "R", "ts": "T", "ny": "W", "gy": "G", "ky": "K", "ch": "C", "hy": "H", "mu": "M", "by":"B", "py": "P", "dy":"D", "ty":"Q", "cl": "X"}
x2_consonant_rev = {v:k for k, v in x2_consonant.items()}


alp = r"[a-zA-Z]+"

def print_diff(text1, text2):
    # difflibì—ì„œ ì œê³µí•˜ëŠ” ë¹„êµ ê°ì²´ ìƒì„±
    text1 = text1.lower()
    text2 = text2.lower()
    d = difflib.Differ()
    diff = list(d.compare(text1, text2))

    # # í‹€ë¦° ë¶€ë¶„ ìƒì„¸í•˜ê²Œ ì¶œë ¥
    print(f"Text1: {''.join(text1)}")
    print(f"Text2: {''.join(text2)}")
    print(diff)
    # # í‹€ë¦° ë¶€ë¶„ ìƒì„¸ ì¶œë ¥

    new_line = ""
    for i, line in enumerate(diff):
        prefix = line[0]
        content = line[2:]
        # if content == "_": continue
        if prefix == '-':
            print(f"Text2 ({i}): {''.join(content)}")
            if re.match(alp, content):
                continue
            elif content == "_":
                continue
            else:
                new_line += "_"+content+"_"
        elif prefix == '+':
            print(f"Text2 ({i}): {''.join(content)}")
            if content == " ": continue

            new_line += content
        else:
            new_line += content
    return new_line

def print_diff2(text1, text2):
    # ë‘ í…ìŠ¤íŠ¸ë¥¼ difflib.SequenceMatcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ
    org_text1 = text1
    org_text2 = text2
    text1 = text1.lower()
    text2 = text2.lower()
    sequence_matcher = difflib.SequenceMatcher(None, text1, text2)
    opcodes = sequence_matcher.get_opcodes()
    
    # ë‘ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê°™ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    aligned_text1 = []
    aligned_text2 = []
    for tag, i1, i2, j1, j2 in opcodes:
        if tag == 'replace':
            for k in range(max(i2 - i1, j2 - j1)):
                if i1 + k < i2:
                    aligned_text1.append(org_text1[i1 + k])
                    # aligned_text1.append("_")
                else:
                    aligned_text1.append('_')
                if j1 + k < j2:
                    aligned_text2.append(org_text2[j1 + k])
                    # aligned_text2.append("_")
                else:
                    aligned_text2.append('_')
        elif tag == 'equal':
            aligned_text1.extend(org_text1[i1:i2])
            aligned_text2.extend(org_text2[j1:j2])
        elif tag == 'delete':
            aligned_text1.extend(org_text1[i1:i2])
            aligned_text2.extend(['_'] * (i2 - i1))
        elif tag == 'insert':
            aligned_text1.extend(['_'] * (j2 - j1))
            aligned_text2.extend(org_text2[j1:j2])
    
    # ë‚¨ì€ ë¶€ë¶„ ì¶”ê°€
    aligned_text1 = ''.join(aligned_text1)
    aligned_text2 = ''.join(aligned_text2)

    # ë‘ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê°™ê²Œ ë§ì¶¤
    max_len = max(len(aligned_text1), len(aligned_text2))
    aligned_text1 = aligned_text1.ljust(max_len, '_')
    aligned_text2 = aligned_text2.ljust(max_len, '_')
    
    return aligned_text1, aligned_text2

def _extract_fullcontext_label(text):

    if V(pyopenjtalk.__version__) >= V("0.3.0"):
        return pyopenjtalk.make_label(pyopenjtalk.run_frontend(text))
    else:
        return pyopenjtalk.run_frontend(text)[1]

def _numeric_feature_by_regex(regex, s):
    match = re.search(regex, s)
    if match is None:
        return -50
    return int(match.group(1))

def pyopenjtalk_g2p_prosody(text: str, drop_unvoiced_vowels: bool = True) -> List[str]:
    """Extract phoneme + prosoody symbol sequence from input full-context labels.

    The algorithm is based on `Prosodic features control by symbols as input of
    sequence-to-sequence acoustic modeling for neural TTS`_ with some r9y9's tweaks.

    Args:
        text (str): Input text.
        drop_unvoiced_vowels (bool): whether to drop unvoiced vowels.

    Returns:
        List[str]: List of phoneme + prosody symbols.

    Examples:
        >>> from espnet2.text.phoneme_tokenizer import pyopenjtalk_g2p_prosody
        >>> pyopenjtalk_g2p_prosody("ã“ã‚“ã«ã¡ã¯ã€‚")
        ['^', 'k', 'o', '[', 'N', 'n', 'i', 'ch', 'i', 'w', 'a', '$']

    .. _`Prosodic features control by symbols as input of sequence-to-sequence acoustic
        modeling for neural TTS`: https://doi.org/10.1587/transinf.2020EDP7104

    """
    def fix_longer_sym(text):
        # ì¥ìŒ ì•ì— ë¹ˆì¹¸ì´ ì˜¤ëŠ” ê²½ìš°ë¥¼ ì œê±°
        return re.sub(r" +ãƒ¼", "ãƒ¼", text)
    text = text.replace('ã€ ', 'ã€').replace('ã€‚ ', 'ã€‚').replace(' ', 'ã€')
    text = fix_longer_sym(text)
    kks = pykakasi.kakasi()
    puncs = [x for x in text if x in SPECIAL_NOTES]
    text = re.sub(SPECIAL_NOTES_PATTERN, " ", text)
    ss = kks.convert(text)
    y = "".join(x['hepburn'].lower() if x['orig'] not in SPECIAL_NOTES else x['orig'] for x in ss).replace("'", "")
    for k, v in x2_consonant.items():
        y = y.replace(k, v)
    y = "_".join([sym if sym != " " else puncs.pop(0) for sym in y ])
    labels = _extract_fullcontext_label(text)
    # puncs = re.findall(r"[ã€‚ã€ï¼Ÿï¼.;ï¼šï¼›:,ï¼Œ:?!~\-ğŸ˜¦ğŸ˜§ğŸ˜ğŸ˜‘ğŸ˜ƒğŸ˜„ğŸ˜”ğŸ˜ğŸ˜«ğŸ˜±ğŸ˜¤ğŸ˜¬ğŸ™ˆğŸ™‰ğŸ¢ğŸŠğŸ‹ ']", text)

    N = len(labels)
    phones = []
    tmp_phones = []
    for n in range(N):
        lab_curr = labels[n]
        # current phoneme
        p3 = re.search(r"\-(.*?)\+", lab_curr).group(1)
        if p3 in x2_consonant:
            p3 = x2_consonant[p3]
        # deal unvoiced vowels as normal vowels
        if drop_unvoiced_vowels and p3 in "AEIOU":
            p3 = p3.lower()
        # deal with sil at the beginning and the end of text
        if p3 == "sil":
            continue
        elif p3 == "pau":
            tmp_phones.append(" ")
            continue
        else:
            phones.append(p3)
            tmp_phones.append(p3)

        # accent type and position info (forward or backward)
        a1 = _numeric_feature_by_regex(r"/A:([0-9\-]+)\+", lab_curr)
        a2 = _numeric_feature_by_regex(r"\+(\d+)\+", lab_curr)
        a3 = _numeric_feature_by_regex(r"\+(\d+)/", lab_curr)

        # number of mora in accent phrase
        f1 = _numeric_feature_by_regex(r"/F:(\d+)_", lab_curr)

        a2_next = _numeric_feature_by_regex(r"\+(\d+)\+", labels[n + 1])
        # accent phrase border
        if a3 == 1 and a2_next == 1 and p3 in "aeiouAEIOUNcl":
            phones.append("#")
        # pitch falling
        elif a1 == 0 and a2_next == a2 + 1 and a2 != f1:
            phones.append("]")
        # pitch rising
        elif a2 == 1 and a2_next == 2:
            phones.append("[")
        
    tmp_phones = "_".join(tmp_phones)
    if y[0] == "ğŸŠ"  and y[-1] == "ğŸ‹":
        tmp_phones = "ğŸŠ_" + tmp_phones + "_ğŸ‹"
    a, b = print_diff2(y, tmp_phones)

    a = list(a)
    b = list(b)

    for i, (_,_) in enumerate(zip(a,b)):
        if a[i] in SPECIAL_NOTES:
            if re.match(r"[a-zA-Z]", b[i] ) is not None:
                b[i] = f"_{a[i]}_{b[i]}"
            else:
                b[i] = a[i]
    b = "".join(b).replace("ã€‚", ".").replace("ï¼Œ", ",").replace("ã€", ",").replace("ï¼Ÿ", "?").replace("ï¼", "!").replace("ï¼š", ",")
    prosodic = "_".join(phones)

    a, b = print_diff2(prosodic, b)

    a = list(a.replace("_", ""))
    b = list(b.replace("_", ""))
    # aê°€ í’€ ìŒì†Œë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— ê°™ì€ ì¸ë±ìŠ¤ì— a[i]ê°€ b[i]ì— ì—†ëŠ” ê²½ìš° ì‚½ì…í•  ê²ƒ.
    new_sequence = list()
    
    i = 0
    j = 0
    burden = 0

    while True:
        # ë‘ ë¬¸ìì—´ ì •ë ¬í•˜ì—¬ prosodic label (#[]) ì‚½ì…
        cur_a = a[i]
        cur_b = b[j]

        if cur_a == cur_b:
            i += 1
            j += 1
            new_sequence.append(cur_a)
        if cur_a in "#[]":
            new_sequence.append(cur_a)
            i += 1
        if cur_b in SPECIAL_NOTES:
            new_sequence.append(cur_b)
            j += 1
        if i >= len(a):
            new_sequence.extend(b[j:])
            break
        elif j >= len(b):
            new_sequence.extend(a[i:])
            break 
        burden += 1
        if burden > 2000:
            # ì •ë ¬ì— ì‹¤íŒ¨í•œ ê²½ìš° (ë³´í†µ ì•ŒíŒŒë²³ ì‹œí€€ìŠ¤ê°€ ë¶ˆì¼ì¹˜ í•˜ëŠ” ê²½ìš°ì„)
            # ì´ ê²½ìš° pyopenjtalk ê²°ê³¼ë¥¼ ë¦¬í„´í•¨
            if y[0] == "ğŸŠ"  and y[-1] == "ğŸ‹":
                out = ["ğŸŠ"] + phones + ["ğŸ‹"]
            else:
                out = phones
            return out

    # ì‰¼í‘œê°€ ì•ë’¤ë¡œ ë”± ë¶™ì€ ê²½ìš°(ì•ŒíŒŒë²³ + ì‰¼í‘œ + ì•ŒíŒŒë²³),
    # ë§¤ë„ëŸ¬ìš´ ë°œìŒì„ ìœ„í•´ ê³µë°± ì¶”ê°€(ì•ŒíŒŒë²³ + ì‰¼í‘œ + ê³µë°± + ì•ŒíŒŒë²³)
    text = ''.join(new_sequence).replace(',,', ',')
    text = re.sub(r'ğŸ¢+', '~', text) # ...ì´ ê±°ë¶ì´ ì‹¬ë³¼ë¡œ ë°”ë€ŒëŠ”ë°, ì¢€ ë” ì˜ ë¨¹ë„ë¡ ~ë¡œ ë°”ê¿”ì¤Œ
    new_sequence = list(re.sub(
        r'(?<=[a-zA-Z\~])([,.])(?=[a-zA-Z])',
        lambda m: m.group(1) + ' ', text # , ë˜ëŠ” . ì¸ ê²½ìš° ë’¤ì— ê³µë°± ë¶™ì´ê¸°
    ))

    return new_sequence


class JpnG2pHolder:
    def __new__(cls, *args, **kwargs):
        obj = super().__new__(cls)
        return obj

    def __init__(self, category, *args, **kwargs):
        self.category = self._validate_category(category)
        self._build_g2p()
        self.g2p = self._set_g2p()

    def _validate_category(self, category):
        assert category == "prosody", logging.info(f"This phoneme category of english is not allowed : {self.category}.")
        return category

    def _build_g2p(self):
        self.g2p_class = {
            "prosody": JpnProG2p()
            }

    def _set_g2p(self):
        return self.g2p_class[self.category]

    def __call__(self, text):
        return self.g2p(text)


class JpnProG2p():
    def __init__(self):
        # self.symb_list = [s for s in SYMBOLS]
        super().__init__()

    def __call__(self, text):
        pronounced = pyopenjtalk_g2p_prosody(text)  # dummy for first punc
        for i in range(len(pronounced)):
            if pronounced[i] not in SPECIAL_NOTES and pronounced[i] != " ":
                if pronounced[i] in x2_consonant_rev:
                    pronounced[i] = x2_consonant_rev[pronounced[i]]
                pronounced[i] = 'jp_' + pronounced[i]
        pronounced = [sym for sym in pronounced if sym != ""]        
        return pronounced
    

if __name__ == "__main__":
    # from nctp.common import remove_bracket
    # from nctp.text_processor import TextProcessor
    # from nctp.common import Language
    # from nctp.common import NormalizeStep
    # import pykakasi
    # from nctp.japanese import convert_number_to_hiragana_in_text
    import kanjize
    import re

    def convert_number_to_hiragana_in_text(text):
        # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì—ì„œ ìˆ«ì ë¶€ë¶„ì„ ì¶”ì¶œ
        def replace_number_with_hiragana(match):
            number = int(match.group(0))
            kanji_str = kanjize.number2kanji(number)
            # hiragana_str = kanjize.kanji2hiragana(kanji_str)
            # return hiragana_str
            return kanji_str
        
        # ëª¨ë“  ìˆ«ì ë¶€ë¶„ì„ ë³€í™˜
        converted_text = re.sub(r'\d+', replace_number_with_hiragana, text)
        return converted_text

    # tp = TextProcessor('japanese_prosody', "default", use_g2p=True)
    g2p = JpnProG2p()
    # # print(tp._symbols)
    # # print(g2p(remove_bracket("ãã®çŸ¥ã‚‰ã›ã‚’èã„ãŸäººã€…ã¯çš†ã€æèˆœè‡£ï¼ˆã‚¤ãƒ»ã‚¹ãƒ³ã‚·ãƒ³ï¼‰ã‚’æ°—ã‚’æ¯’ã«æ€ã£ãŸã€‚")))
    kks = pykakasi.kakasi()
    # result = kks.convert("ã“ã‚Œã‚‰ã®å¤šãã¯1950å¹´ä»£ã«ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼, ã‚¹ã‚¿ã‚¸ã‚ªã®ä¸»è¦ãƒ©ã‚¤ãƒãƒ«ã§ã‚ã£ãŸUPAã®è¨­ç«‹ã«å‚åŠ ã—ãŸã€‚")
    # print(result)
    # # print(g2p(remove_bracket("ãã®çŸ¥ã‚‰ã›ã‚’èã„ãŸäººï¼Ÿã€…ã¯çš†ã€æèˆœè‡£ï¼ˆã‚¤ãƒ»ã‚¹ãƒ³ã‚·ãƒ³ï¼‰ã€ã‚’æ°—ã‚’æ¯’ã«ï¼æ€ã£ãŸã€‚")))
    # print(tp.symbolize(g2p(remove_bracket("é€“ä¼å“¨ã«ã¯ã€ä¹—é¦¬ä¼ä»¤ã«ã‚ˆã‚‹é€“é¨å“¨ã€è‡ªè»¢è»Šã«ã‚ˆã‚‹é€“è‡ªè»¢è»Šå“¨ãªã©ã®ç¨®é¡ãŒã‚ã‚‹."))))
    # print(g2p("å›ãŒã„ãªã‘ã‚Œã°ã¾ãŸé‡‘åºŠã®å‰ã« ç«‹ã¤ã“ã¨ã¯ç„¡ã‹ã£ãŸã ã‚ã†ï¼ ä»»ã›ã¦ãã‚Œã€ ãƒ™ã‚¹ãƒˆã‚’å°½ããã†ï¼"))
    # text = "ã‹ã™ã‹ã«èã“ãˆã¦ãã‚‹~1931å¹´ç‰ˆã®è®ƒç¾æ­Œ~ãŒ? æ¬¡ç¬¬ã«å¤§ãããªã£ã¦ã„ã."
    # text = "ãƒ«ãƒ¼ãƒ«ãƒ¼ã‚³ã‚¦ã¨ã„ã†åã¯,ä¸­å›½ã«ã¯ãªã„."
    # # norm_text = convert_number_to_hiragana_in_text(text)
    # # # norm_text = tp.normalize("åŒã˜æ–™ç†ã˜ã‚ƒãªã„ã§ã™ã‚ˆã€‚ ã†ã¡ã®æ–™ç†ã®æ–¹ãŒç¾å‘³ã—ã„ã§ã™ï¼")
    # # # norm_text = tp.normalize(text)
    # # norm_text = tp.clean(norm_text)
    # print(norm_text)
    # pron_text = tp.pronounce(norm_text)
    # sym = tp.symbolize(pron_text)
    # print(sym)
    # sym = tp.input2symbol(text)
    # print(norm_text)
    
    # text = " "
    print(g2p("ãƒ«ãƒ¼ãƒ«ãƒ¼ã‚³ã‚¦ã¨ã„ã†åã¯,ä¸­å›½ã«ã¯ãªã„."))
    

    # print(kks.convert(text))
    # print(tp.input2symbol(text))
    # x = "".join(x['hepburn'] for x in kks.convert("ç§‹è‘‰åŸã«ã¯,é›»æ°—å±‹ãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™. "))
    # print(x)
    # print(g2p(" å›ã¯èª°ã§ã™ã‹?"))
    # print(pyopenjtalk.g2p(" å›ã¯èª°ã§ã™ã‹ï¼Ÿ"))
    # print(g2p("å›ã¯èª°ã§ã™ã‹?"))
    # # print(g2p("".join(item['hira'] for item in result)))
    # for f in pyopenjtalk.run_frontend("ã‚ã€é€“ä¼å“¨ã«ã¯ã€ä¹—é¦¬ä¼ä»¤ã«ã‚ˆã‚‹é€“é¨å“¨ã€è‡ªè»¢è»Šã«ã‚ˆã‚‹é€“è‡ªè»¢è»Šå“¨ãªã©ã®ç¨®é¡ãŒã‚ã‚‹."):
    # import pykakasi
    # kks = pykakasi.kakasi()
    # x = ""
    # for f in pyopenjtalk.run_frontend("ä½•ã®äººå½¢ãŒå‡ºã¦ãã‚‹ã‹ã€ã‚ã‹ã‚Šã¾ã›ã‚“! ãã‚Œã§ã‚‚è²·ã„ã¾ã™ã‹ï¼Ÿ"):
    #     x += " "+kks.convert(f['pron'])[0]['hepburn'] 
    #     # print(f)
    #     print(f['string'], f['pron'], kks.convert(f['pron'])[0]['hepburn'])
    # print(x)
    # y = ""
    # for r in pyopenjtalk.g2p("!!ä½•å›ã¯èª°ã§ã™ã‹ï¼Ÿ").split(" "):
    #     y += r if r != "pau" else ""
    # print(y)
    # print(pyopenjtalk.run_frontend("é€“ä¼å“¨ã«ã¯ã€ä¹—é¦¬ä¼ä»¤ã«ã‚ˆã‚‹é€“é¨å“¨ã€è‡ªè»¢è»Šã«ã‚ˆã‚‹é€“è‡ªè»¢è»Šå“¨ãªã©ã®ç¨®é¡ãŒã‚ã‚‹."))