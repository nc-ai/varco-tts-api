import re
import logging
import jamo

import unicodedata
from enum import Enum
from typing import Callable
from typing import Dict
from typing import Union
from typing import List

import nctp.korean as knorm
import nctp.english as enorm
import nctp.chinese as cnorm
import nctp.taiwanese as tnorm
import nctp.japanese as jnorm
from nctp.symbols import CommonSymbols, ERR_SYMBOL, SpecialSymbols

TAG_DICT = {'@p': 'player', '@t': 'team', '@f': 'break', '@i': 'interjection', '@in': 'sigh'}

WHITESPACE_PATTERN = [
    [r'\xa0{1,}', ' '], [r'\s{1,}', ' '], [r'\!{4,}', '!'], [r'\?{4,}', '?'],
    [r'\.{4,}', '...'], [r'\,{2,}', ','], [r'\'{2,}', '\''], [r'\ã€œ{1,}', '~'], [r'\~{3,}', '~~']
]
# WHITESPACE_PATTERN_SERVICE = [
#     # ëŠë‚Œí‘œë¥¼ ì œì™¸í•œ ë¬¸ì¥ë¶€í˜¸ëŠ” í•˜ë‚˜ë§Œ ì‘ì„±í•˜ëŠ” í¸ì´ ì„±ëŠ¥ ìš°ìˆ˜í•¨.
#     # ë¬¼ìŒí‘œëŠ” 2ê°œ ì´ìƒ ì‚¬ìš© ì‹œ ì—´í™” ë“±ì˜ ì´ìŠˆ

#     [r'(\xa0|\s|\_){1,}', r' '],
#     [r'\.{2,}(\!|\?)', r'...\1'], [r'(\?\!){1,}', '?!'], [r'(\!\?){1,}', '!?'],
#     [r'\,{2,}', ','], [r'\'{2,}', '\''],
#     # [r'\~+\?+', r'~?'], [r'\~+\!+', r'~!'],
#     [r'\ã€œ{1,}', '~'], [r'\~{2,}', '~'],
#     [r'\;{1,}', '...'], [r'\.{2,}', '...'], [r'\!{3,}', '!!'], [r'\?{3,}', '??']
# ]

TAG_SPLIT_PATTERN = re.compile(r"\[(\w)\]([^[\[\/?\]]+)\[(\/)(\w)\]")

TAG_PATTERN = re.compile(r"\[(\/?)(\w)\]")

brackets_wtext = re.compile(r"\([^)]+\)")
brackets = re.compile(r"[\(\)]+")

quotation = re.compile(r"[ã€Œã€\"\'\â€œ\â€]+")

CONTMARKS_WHITE_LIST = [
    # r'\.\.\.\?',  # ...? model capacity issue
    # r'\.\.\.!',   # ...!
    # r'\.\.\.',    # ...
    r'\~\.',      # ~.
    r'\?\!',      # ?!
    # r'\!\?',      # !? model capacity issue
    r'\!\!',      # !!
    # r'\?\?',      # ?? model capacity issue
    r'\~\?',      # ~?
    r'\~\!',      # ~!
    r'\.\'',      # .'
    r'\!', r'\?', r'\~', r'\.', r'\_', r'\'', r'\,', r'\-'
]


def service_pattern_generator(white_list):
    tails = '['
    for key in CommonSymbols().sym2num.keys():
        if key == ' ':
            continue
        tails += '\\' + key
    tails += ']*'

    heads = '|'.join(white_list)
    heads = r'({})'.format(heads)
    pattern = [heads + tails, r'\1']
    return pattern


WHITESPACE_PATTERN_SERVICE = [
                   [r'\ã€œ{1,}', '~'], [r'\~{2,}', '~'],
                   [r'(\S)[\_\çµ‚\'\,\-\.\å§‹\!]+(\?){1,}', r'\1\2'], # model capapcity issue
                   [r'(\S[^(\_\çµ‚\~\!\'\,\-\.\?\å§‹)])[\.]+(\!)', r'\1\2'],
                   [r'\;{1,}', '.'], [r'\.{2,}', '.'],
                   [r'\,{2,}', ','], [r'\'{2,}', '\''],
                   service_pattern_generator(CONTMARKS_WHITE_LIST),
                   [r'(\xa0|\s|\_){1,}', r' ']
]
# model capacity issue
# [r'\;{1,}', '...'], [r'\.{2,}', '...'],
# [r'(\S[^(\_\çµ‚\~\!\'\,\-\.\?\å§‹)])\.(\!|\?)', r'\1\2'], # model capacity issue : origin => (\!|\?)


def parse_styles(text):
    '''
    Parse styles

    Args:
        text (str): text for parsing

    Returns:
        style_dict (dict): dictionary of style
        text (str): remained text excluding style rule

    Examples:
        >>> parse_styles('<-style:5&reverb:0->ì•ˆë…•í•˜ì„¸ìš”')
        ({'style':5, 'reverb':0}, 'ì•ˆë…•í•˜ì„¸ìš”')
        >>> parse_styles('<-reverb:0.1-> ì…ë‹¥ì³ ë§í¬ì´')
        ({'reverb':0.1}, ' ì…ë‹¥ì³ ë§í¬ì´')
        >>> parse_styles('  <-reverb:0-> ì…ë‹¥ì³ ë§í¬ì´')
        ({'reverb':0}, ' ì…ë‹¥ì³ ë§í¬ì´')
        >>> parse_styles('ìœ„ì¦ë¦¬! <-reverb:0-> ì…ë‹¥ì³ ë§í¬ì´')
        ({'reverb':0}, 'ìœ„ì¦ë¦¬! <-reverb:0-> ì…ë‹¥ì³ ë§í¬ì´')
    '''
    pattern = r'^\<\-(\w+\:(\d+([.,]\d*)?|[.,]\d+)([eE][-+]?\d+)?\&?)+\-\>'
    style_dict = {'style': 0, 'reverb': 0., 'duration': 0., 'speed': 1., 'pitch': 1., 'energy': 1.}
    result = re.search(pattern, text.strip())
    if result is not None:
        styles = result.group()[2:-2].split('&')
        for style in styles:
            key, value = style.split(':')
            if key == 'style':
                style_dict[key] = int(value)
            elif key == 'reverb':
                style_dict[key] = float(value)
            elif key == 'duration':  # TODO: it will be deprecated
                style_dict[key] = float(value)
            elif key == 'speed':
                style_dict[key] = float(value)
            elif key == 'pitch':
                style_dict[key] = float(value)
            elif key == 'energy':
                style_dict[key] = float(value)
        text = result.string[result.span()[-1]:]  # separate remained text

    # pattern = re.compile(r'\@[ê°€-í£]{1,2}[\.|\!]+|[ê°€-í£]+\@i{1}n{1}|[ê°€-í£]+\@[ptfi]')

    # def at_style(founded):
    #     at_index = founded.group().index('@')
    #     if at_index == 0:
    #         text = founded.group()[1:]
    #         style_dict['at_style'].append((text, 'interjection'))
    #     else:
    #         key = founded.group()[at_index:]
    #         if key in TAG_DICT.keys():
    #             text = founded.group()[:at_index]
    #             style_dict['at_style'].append((text, TAG_DICT[key]))
    #     return text

    # text = re.sub(pattern, lambda x: at_style(x), text)

    return style_dict, text

def handle_for_correct_puncs(input: str):
    # PUNCS_AFTER_SPACE = r"([ğŸ¢~ã€‚ã€ï¼Ÿï¼.;ï¼›:,ï¼Œ:?!])(?=[^ğŸ˜§ğŸ˜‘ğŸ˜„ğŸ˜ğŸ˜±ğŸ˜¬ğŸ™‰ğŸ‹ğŸ¤ğŸ˜¢ğŸ¢~ã€‚ã€ï¼Ÿï¼.;ï¼›:,ï¼Œ:?! ])(?!$)"
    PUNCS_AFTER_SPACE = r"([ğŸ¢~ã€‚ã€ï¼Ÿï¼.;ï¼›:,ï¼Œ:?!])(?!(\d\.\d))(?!(\d))(?=[^ğŸ˜§ğŸ˜‘ğŸ˜„ğŸ˜ğŸ˜±ğŸ˜¬ğŸ™‰ğŸ‹ğŸ¤ğŸ˜¢ğŸ¢~ã€‚ã€ï¼Ÿï¼.;ï¼›:,ï¼Œ:?!\s])(?!$)" # ìˆ«ì ì†Œìˆ˜ì  ì œì™¸
    MORE_SPACES = r"\s{2,}"
    result = re.sub(PUNCS_AFTER_SPACE, r'\1 ', input)
    result = re.sub(MORE_SPACES, " ", result).strip()
    result = re.sub(r'\s*ğŸ¤\s*', 'ğŸ¤', result) # break ì‹¬ë³¼ ì–‘ ì˜†ì— SPACE í† í° ìˆëŠ”ì§€ íƒìƒ‰
    result = re.sub(r'\s*ğŸ‹', 'ğŸ‹', result) # voice control end ì•ì— ê³µë°±ì´ ìˆìœ¼ë©´ ì œê±°
    result = re.sub(rf'ğŸ‹(?!\s|$)', 'ğŸ‹ ', result) # voice control end ë’¤ì— ê³µë°±ì´ ì—†ê±°ë‚˜ ëì´ ì•„ë‹ˆë©´ ì¶”ê°€
    return result

def parse_tagger(input: str) -> Union[str, List]:
    # NOTE: UPDATED BY MKYU (24.02.19)
    """
        input: tagging ì´ í‘œê¸°ëœ ë¬¸ì¥ ex) [g]ìŒ...[/g] [s]ì•„, ì•„,[/s] ê·¸ê²Œ, [g]ê·¸...[/g] ì•ˆë…•í•˜ì„¸ìš”?"
        tagëŠ” ì¤‘ê´„í˜¸ ì•ˆì— ì•ŒíŒŒë²³ nê°œë¡œ êµ¬ì„±
        output:
            - pure_text(str): tagê°€ ì œê±°ëœ ë¬¸ì¥
            - pure_tag(list): tagë¥¼ ì§€ë‹ˆëŠ” ë¦¬ìŠ¤íŠ¸
            - input(str): not processed text
    """
    text_list = list(input)
    res = TAG_SPLIT_PATTERN.finditer(input)
    tag_list = [0] * len(input)

    for x in res:
        start, end = x.start(), x.end()
        tag_list[start:end] = [x.group(1)] * (end - start)
        text_list[start: start + len(x.group(1)) + 2] = ["_"] * ((start + len(x.group(1)) + 2) - start)
        text_list[end - len(x.group(1)) - 3: end] = ["_"] * ((end + len(x.group(1)) + 3) - end)

    if "_" in text_list:
        pure_text = []
        pure_tag = []
        for idx in range(len(text_list)):
            if text_list[idx] != "_":
                pure_text.append(text_list[idx])
                pure_tag.append(tag_list[idx])
        pure_text = ''.join(pure_text)
    else:
        pure_text, pure_tag = input, tag_list
    return pure_tag, pure_text, input

def parse_style_tag_indi(input: str):
    # NOTE: CREATED BY MKYU (24.02.19)
    """
        input: tagging ì´ í‘œê¸°ëœ ë¬¸ì¥ ex) [g]ìŒ...[/g] [s]ì•„, ì•„,[/s] ê·¸ê²Œ, [g]ê·¸...[/g] ì•ˆë…•í•˜ì„¸ìš”?"
        outout:
            íƒœê·¸ê°€ ì‹¬ë³¼ë¡œ ë°”ë€ ë¬¸ì¥ ex) ğŸ˜¦ìŒ...ğŸ˜§ ğŸ˜ì•„, ì•„,ğŸ˜‘ ê·¸ê²Œ, ğŸ˜¦ê·¸...ğŸ˜§ ì•ˆë…•í•˜ì„¸ìš”?
    """
    ssym = SpecialSymbols()
    res = TAG_PATTERN.finditer(input)
    for r in res:
        input = input.replace(r.group(0), ssym.tag2sym.get(r.group(0),r.group(0)), 1) #ìŠ¤íƒ€ì¼ íƒœê·¸ì— í•´ë‹¹ ë˜ì§€ ì•ŠëŠ” íƒœê·¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜ ë˜ë„ë¡ ìˆ˜ì • - 25/03/25 dongjoo195
    return input

def parse_style_tag2ssml(input: str):
    # ì…ë ¥ë°›ì€ ë¬¸ì¥ì—ì„œ style tagë¥¼ ssmlë¡œ íŒŒì‹±í•©ë‹ˆë‹¤(ë³´ì—¬ì£¼ê¸°ìš©ì„, íê¸°ì˜ˆì •)
    """
        input: tagging ì´ í‘œê¸°ëœ ë¬¸ì¥ ex) [g]ìŒ...[/g] [s]ì•„, ì•„,[/s] ê·¸ê²Œ, [g]ê·¸...[/g] ì•ˆë…•í•˜ì„¸ìš”?"
        outout:
            íƒœê·¸ê°€ ì‹¬ë³¼ë¡œ ë°”ë€ ë¬¸ì¥ ex) ğŸ˜¦ìŒ...ğŸ˜§ ğŸ˜ì•„, ì•„,ğŸ˜‘ ê·¸ê²Œ, ğŸ˜¦ê·¸...ğŸ˜§ ì•ˆë…•í•˜ì„¸ìš”?

    """
    ssym = SpecialSymbols()
    res = TAG_PATTERN.finditer(input)
    
    nv_type = {
        "g": "ê°„íˆ¬ì–´",
        "s": "ë§ë”ë“¬",
        "l": "ì›ƒìŒ",
        "c": "ë¹„ëª…",
        "h": "ê¸°í•©",
        "i": "í•œìˆ¨",
        "r": "ìš¸ìŒ"
    }

    for r in res:
        t = r.group(0)[1]
        if t == "/":
            replace = '</nc:texteffect>'
        else:
            replace = f'<nc:texteffect type="{nv_type[t]}">'  
        input = input.replace(r.group(0), replace, 1)
    return input

def remove_bracket(input: str):
    # NOTE: CREATED BY MKYU (24.02.20)
    """
        input: ê´„í˜¸ ì—¬ë‹«ìŒì´ ìˆëŠ” ë¬¸ì¥ (ì£¼ë¡œ ë³‘ìŒí‘œê¸°ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤) ex) ì €ëŠ” ì œì„ìŠ¤(James)ì™€ ì½œë¦°(Colin) ìš”ì›ì„ ê°€ì¡±ìœ¼ë¡œ ìƒê°í•˜ê³  ((ìˆìŠµë‹ˆë‹¤.
        output: ê´„í˜¸ ì—¬ë‹«ìŒì´ ì—†ëŠ” ë¬¸ì¥ ex) ì €ëŠ” ì œì„ìŠ¤ì™€ ì½œë¦° ìš”ì›ì„ ê°€ì¡±ìœ¼ë¡œ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    """
    fixed_code = input.replace("ï¼ˆ", "(").replace("ï¼‰", ")").replace("[", "").replace("]", "")

    bracket_wtext_removed = re.sub(brackets_wtext, "", fixed_code)
    bracket_removed = re.sub(brackets, "", bracket_wtext_removed)
    output = bracket_removed.replace("  ", " ")
    return output

def remove_quotation(input: str):
    # NOTE: CREATE BY MKYU (24.02.20)
    """
        input: ì¸ìš©í‘œì‹œê°€ ìˆëŠ” ë¬¸ì¥ 
        output: ì¸ìš©í‘œì‹œë¥¼ ì œê±°í•œ ë¬¸ì¥
    """
    return re.sub(quotation, "", input)

def convert_enumeration(input: str):
    # NOTE: CREATE BY MKYU (24.02.20)
    """
        input: ì¤‘êµ­ì–´, ì¼ë³¸ì–´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°€ìš´ë° ì ì„ ì‚¬ìš©í•œ ì—´ê±°í˜• ë¬¸ì¥ ex) ë„¤ê°€ ì¥ì„ ë´ì•¼í•  ê²ƒì€ í† ë§ˆí† ãƒ»ë°”ë‚˜ë‚˜ãƒ»ê°ìãƒ»ì¹˜ì¦ˆãƒ»ì†Œê³ ê¸° ë€ë‹¤.
        output: ê°€ìš´ë° ì ì„ ë°˜ì  (", ") ìœ¼ë¡œ ì¹˜í™˜í•¨. (ë°˜ì +SPACE)
    """
    return input.replace("ãƒ»", ", ")

def strip_diacritics(input: str):
    return ''.join(char for char in unicodedata.normalize('NFD', input) if unicodedata.category(char) != 'Mn')

def convert_ellipsis(text: str) -> str:
    # ëª‡ëª‡ í…ìŠ¤íŠ¸ í¸ì§‘ê¸°ì—ì„œ ...ì„ ì••ì¶•í•˜ì—¬ í‘œí˜„í•˜ëŠ” ê²ƒ ê°™ë‹¤.
    text = re.sub("â€¦", "ğŸ¢", text)
    text = re.sub(r"\.\.\.+", "ğŸ¢", text)
    text = re.sub(r"\.\.", ".", text)
    return text

def collapse_linebreak(text: str) -> str:
    '''ì—¬ëŸ¬ ë¼ì¸ì˜ ë¬¸ì¥ì´ ë“¤ì–´ì˜¬ ê²½ìš° linebreakë“¤ì„ ì—†ì•  í•œ ë¼ì¸ìœ¼ë¡œ collapse.

    Args:
        text (str): ì…ë ¥ ë¬¸ì¥

    Returns:
        text (str): ì¤‘ë³µ ë¬¸ì¥ ë¶€í˜¸ ì œê±° ì²˜ë¦¬ ëœ ê²°ê³¼
    '''
    if text.count('\n') == 0:
        return text

    lines = (line.strip() for line in text.splitlines())
    lines = (knorm.join_period(line) for line in lines if line != '')
    return ' '.join(lines)


def collapse_specialchars(text, custom_pattern=None):
    """ ì…ë ¥ ë¬¸ì¥ ë‚´ ì¤‘ë³µ ê³µë°± í˜¹ì€ ì¤‘ë³µ ë¬¸ì¥ ë¶€í˜¸ ì œê±°
        | ë¬¸ì¥ ë¶€í˜¸ | ì¤‘ë³µ ê°œìˆ˜(Nê°œ ì´ìƒ) | ì²˜ë¦¬ |
        | (space)  | 1 | í•˜ë‚˜ë¡œ í†µí•© |
        | ! | 4 | í•˜ë‚˜ë¡œ í†µí•© |
        | ? | 4 | í•˜ë‚˜ë¡œ í†µí•© |
        | . | 4 | ë§ì¤„ì„í‘œ(ë§ˆì¹¨í‘œ ì„¸ê°œ)ë¡œ ë³€í™˜
        | , | 1 | í•˜ë‚˜ë¡œ í†µí•© |
        | ' | 2 | í•˜ë‚˜ë¡œ í†µí•© |
        | âˆ¼ | 1 | ~ë¡œ ë³€í™˜ ë° í†µí•© |
        | ~ | 3 | ë‘ê°œë¡œ ë³€í™˜ |

    Args:
        text (str): ì…ë ¥ ë¬¸ì¥

    Returns:
        text (str): ì¤‘ë³µ ë¬¸ì¥ ë¶€í˜¸ ì œê±° ì²˜ë¦¬ ëœ ê²°ê³¼
    """
    # text = re.sub(r'\xa0', ' ', text)
    # _whitespace_re = re.compile(r'\s+')
    # text = re.sub(_whitespace_re, ' ', text)
    if custom_pattern is not None:
        pattern_dict = custom_pattern
    else:
        pattern_dict = WHITESPACE_PATTERN
    for p in pattern_dict:
        text = re.sub(re.compile(p[0]), p[1], text)
    return text


def remove_parentheses(text):
    text, _ = _remove_parentheses(text)
    return text


def _remove_parentheses(text, start=False):
    """ê´„í˜¸ ë‚´ ë‹¨ì–´ ì‚­ì œ

    Args:
        text (str): ì…ë ¥ ë¬¸ì¥
        start (bool, optional): [description]. Defaults to False.

    Returns:
        processed_text (str) : ê´„í˜¸ê°€ ì œê±°ëœ ë¬¸ì¥, ì¬ê·€ í˜¸ì¶œ í›„ ìµœì¢… ê²°ê³¼ëŠ” ê´„í˜¸ì™€ ê´„í˜¸ ë‚´ ë¬¸ìê°€ ì œê±°ëœ ë¬¸ì¥.
        i (int) : pivot í¬ì§€ì…˜
    """
    ret = []
    i = 0
    while i < len(text):
        if text[i] == "(":
            _ret, _i = _remove_parentheses(text[i + 1:], True)
            if _ret == "":
                ret.append(_ret)
                i = i + _i + 1
            else:
                ret.append(text[i])
                ret.append(_ret)
                i = i + _i + 1
        elif text[i] == ")" and start is True:
            return "", i + 1
        else:
            ret.append(text[i])
            i = i + 1
    processed_text = "".join(ret)
    return processed_text, i


# if __name__ == '__main__':
#     print(sym2num)

def log_csv(texts, name, stepbystep=False):
    with open("./logs/{}.csv".format(name), "a", encoding="UTF-8") as f:
        for text in texts:
            if stepbystep:
                f.write('{:40}'.format(text[0]) + ': ' + text[1] + '\n')
            else:
                f.write(text + '\n')
        f.write('\n')


def limit_txtlen(text, limit):
    try:
        if len(text) > limit:
            logging.debug("The length of input text exceed limitation. Length : {}".format(len(text)))
            return text[:limit]
        else:
            return text
    except Exception as err:
        logging.debug(err, exc_info=True)
        return logging.debug("Fail during cheking the length of input text.")


# TODO: Code-Switching


class Language(Enum):
    korean = 'korean'
    korean_ipa = 'korean_ipa'
    english = 'english'
    multi = 'multi'
    english_arpabet = 'english_arpabet'
    english_ipa = 'english_ipa'
    japanese_prosody = 'japanese_prosody'
    chinese = 'chinese'
    taiwanese = 'taiwanese'

    def __str__(self):
        return self.value


class Normalizer:
    def __init__(self, normalize: Callable, *args):
        self._normalize = normalize
        self._args = args

    def normalize(self, target: str) -> str:
        return self._normalize(target, *self._args)


class NormalizeStep(Enum):
    # Common normalize steps
    collapse_linebreak = Normalizer(collapse_linebreak)
    remove_parentheses = Normalizer(remove_parentheses)
    collapse_special_characters = Normalizer(collapse_specialchars)
    collapse_special_characters_service = Normalizer(collapse_specialchars, WHITESPACE_PATTERN_SERVICE)
    handle_style_tag = Normalizer(parse_style_tag_indi)
    convert_ellipsis = Normalizer(convert_ellipsis)
    handle_puncs_spaces = Normalizer(handle_for_correct_puncs)

    # Korean normalize steps
    drop_incompletes = Normalizer(knorm.drop_incompletes)
    patterns = Normalizer(knorm.normalize_patterns)
    number = Normalizer(knorm.normalize_number)
    etc_dictionary = Normalizer(knorm.normalize_with_dictionary, 'etc', 'key_only')
    universe_dictionary = Normalizer(knorm.normalize_with_dictionary, 'universe', 'chunks_upper')
    eng_dictionary = Normalizer(knorm.normalize_with_dictionary, 'english', 'chunks_upper')
    english = Normalizer(knorm.normalize_english)
    character = Normalizer(knorm.normalize_character)
    pronunciation = Normalizer(knorm.normalize_pronunciation)
    period = Normalizer(knorm.join_period)

    # English normalize steps
    to_ascii = Normalizer(enorm.convert_to_ascii)
    lowercase = Normalizer(enorm.lowercase)
    expand_numbers = Normalizer(enorm.expand_numbers)
    expand_abbreviations = Normalizer(enorm.expand_abbreviations)

    # Chinese step
    chn_normalize = Normalizer(cnorm.chn_normalize)
    remove_prosody = Normalizer(cnorm.remove_prosody)
    chn_prosody = Normalizer(cnorm.prosody_predict)
    chn_baker = Normalizer(cnorm.handle_baker_like)

    remove_quotation = Normalizer(remove_quotation)
    convert_enumeration = Normalizer(convert_enumeration)
    remove_bracket = Normalizer(remove_bracket)

    # Taiwanese step
    twn_normalize = Normalizer(tnorm.twn_normalize)
    twn_normalize_new = Normalizer(tnorm.twn_normalize_new)
    twn_prosody = Normalizer(tnorm.prosody_predict)
    twn_baker = Normalizer(tnorm.handle_baker_like)

    # Japanese Normalize
    jpn_num_normalize = Normalizer(jnorm.convert_number_to_hiragana_in_text)



class Cleaner:
    def __init__(self, clean: Callable, *args):
        self._clean = clean
        self._args = args

    def clean(self, target: str) -> str:
        return self._clean(target, *self._args)


class CleanStep(Enum):
    clean_residual = Cleaner(knorm.remove_residual)


def key_checker(options: Dict, target_key: str, default):
    return True if target_key in options else default


def check_then_symbolize(text: str, symbols: Dict, err_symbol=ERR_SYMBOL):
    return [symbols[s] if key_checker(symbols, s, False) else err_symbol for s in text]


def korean_symbolize(text, kor_symbols: Dict, options: Dict):
    """ì •ê·œí™”ëœ ë¬¸ì¥ì˜ ëì— eosë¥¼ ì¶”ê°€í•˜ê³ , symbolí™”"""
    _del_ng = key_checker(options, 'del_ng', False)
    _head = key_checker(options, 'head', False)
    _tail = key_checker(options, 'tail', False)
    text = jamo.h2j(text)

    comsym = CommonSymbols()
    symbolized = check_then_symbolize(text, kor_symbols)

    if _head:
        symbolized = [kor_symbols[comsym.bos], kor_symbols[comsym.space]] + symbolized
    if _del_ng:
        symbolized = list(filter(lambda x: x != kor_symbols[chr(0x110b)], symbolized))
    if _tail:
        symbolized = symbolized + [kor_symbols[comsym.space]]
    # default step
    symbolized = symbolized + [kor_symbols[comsym.eos]]

    return symbolized


def korean_phn_symbolize(text, kor_symbols: Dict, options: Dict):
    """ì •ê·œí™”ëœ ë¬¸ì¥ì˜ ëì— eosë¥¼ ì¶”ê°€í•˜ê³ , symbolí™”"""
    _del_ng = key_checker(options, 'del_ng', False)
    _head = key_checker(options, 'head', False)
    _tail = key_checker(options, 'tail', False)
    text = jamo.h2j(text)
    if _del_ng:
        text = list(filter(lambda x: x != chr(0x110b), text))
    text = [knorm.KR_IPA_MAP[s]["ipa"] if s in knorm.KR_IPA_MAP else s for s in text]

    comsym = CommonSymbols()
    symbolized = check_then_symbolize(text, kor_symbols)
    # symbolized = [kor_symbols[s] for s in text if s in kor_symbols]
    if _head:
        symbolized = [kor_symbols[comsym.bos], kor_symbols[comsym.space]] + symbolized
    # if _del_ng:
    #     symbolized = list(filter(lambda x: x != 311, symbolized))
    if _tail:
        symbolized = symbolized + [kor_symbols[comsym.space]]
    # default step
    symbolized = symbolized + [kor_symbols[comsym.eos]]

    return symbolized


def eng_symbolize(text: Union[str, List], eng_symbols: Dict, options: Dict):
    """ì •ê·œí™”ëœ ë¬¸ì¥ì˜ ëì— eosë¥¼ ì¶”ê°€í•˜ê³ , symbolí™”"""
    _head = key_checker(options, 'head', False)
    _tail = key_checker(options, 'tail', False)

    comsym = CommonSymbols()
    symbolized = check_then_symbolize(text, eng_symbols)
    if _head:
        symbolized = [eng_symbols[comsym.bos], eng_symbols[comsym.space]] + symbolized
    if _tail:
        symbolized = symbolized + [eng_symbols[comsym.space]]
    # default step
    symbolized = symbolized + [eng_symbols[comsym.eos]]

    return symbolized

def jpn_symbolize(text: Union[str, List], jpn_symbols: Dict, options: Dict):
    """ì •ê·œí™”ëœ ë¬¸ì¥ì˜ ëì— eosë¥¼ ì¶”ê°€í•˜ê³ , symbolí™”"""
    _head = key_checker(options, 'head', False)
    _tail = key_checker(options, 'tail', False)
    # print(jpn_symbols)
    comsym = CommonSymbols()
    symbolized = check_then_symbolize(text, jpn_symbols)
    if -1 in symbolized:
        print(text)
    if _head:
        symbolized = [jpn_symbols[comsym.bos], jpn_symbols[comsym.space]] + symbolized
    if _tail:
        symbolized = symbolized + [jpn_symbols[comsym.space]]
    # default step
    symbolized = symbolized + [jpn_symbols[comsym.eos]]

    return symbolized

def chn_symbolize(text: Union[str, List], chn_symbols: Dict, options: Dict):
    """ì •ê·œí™”ëœ ë¬¸ì¥ì˜ ëì— eosë¥¼ ì¶”ê°€í•˜ê³ , symbolí™”"""
    _head = key_checker(options, 'head', False)
    _tail = key_checker(options, 'tail', False)

    comsym = CommonSymbols()
    if type(text) == str:
        text = text.split(" ")
    # new_text = list()
    # for sym in text:
    #     if "#" in sym and "_" in sym:
    #         new_text.extend([sym[:-1], " "])
    #     else:
    #         new_text.append(sym)
    # text = new_text

    symbolized = check_then_symbolize(text, chn_symbols)
    if _head:
        symbolized = [chn_symbols[comsym.bos], chn_symbols[comsym.space]] + symbolized
    if _tail:
        symbolized = symbolized + [chn_symbols[comsym.space]]
    # default step
    symbolized = symbolized + [chn_symbols[comsym.eos]]

    return symbolized

def multi_symbolize(text: Union[str, List], multilang_symbols: Dict, options: Dict):
    """
        í•œêµ­ì–´ symbolize ê³¼ì •ì„ ë”°ë¦„
    """
    return korean_symbolize(text, multilang_symbols, options)



class Symbolizer:
    def __init__(self, symbolize: Callable, *args):
        self._symbolize = symbolize
        self._args = args

    def _validate(self, symbolized: List) -> bool:
        err_flag = True if ERR_SYMBOL in symbolized else False
        if err_flag:
            logging.warning("Error in results of symbolize, please check KeyError in symbolize function.")
            print(symbolized)
        return symbolized

    def symbolize(self, target, symbols: List, options: List = []) -> List:
        return self._validate(self._symbolize(target, symbols, options))


class GetSymbolizer:
    def __init__(self):
        self._symbol_step_dict = {
            Language.korean : Symbolizer(korean_symbolize),
            Language.korean_ipa : Symbolizer(korean_phn_symbolize),
            Language.english : Symbolizer(eng_symbolize),
            Language.multi : Symbolizer(multi_symbolize),
            Language.english_arpabet : Symbolizer(eng_symbolize),
            Language.english_ipa : Symbolizer(eng_symbolize),
            Language.japanese_prosody : Symbolizer(jpn_symbolize),
            Language.chinese : Symbolizer(chn_symbolize),
            Language.taiwanese: Symbolizer(chn_symbolize)
        }

    def __call__(self, lang: Language):
        return self._symbol_step_dict[lang]


def symbolizer_selector(lang: Language):
    __SymbStep = GetSymbolizer()
    return __SymbStep(lang)


if __name__ == "__main__":
    # print(convert_ellipsis("ì•ˆë…•........ í•˜ì„¸ìš”?"))
    from bs4 import BeautifulSoup
    import textwrap
    x = parse_style_tag2ssml("<speak>[g]ìŒ...[/g] [s]ì•„, ì•„,[/s] ê·¸ê²Œ, [g]ê·¸...[/g] ì•ˆë…•í•˜ì„¸ìš”?</speak>")
    x = BeautifulSoup(x, 'xml')
    x = x.prettify()
    print(x)
    print(parse_style_tag_indi("ì˜¤ëŠ˜ ë°°ìš¸ ë‚´ìš©ì€ ìˆ˜í•™ ìµí˜ì±… ë°±ì‚¬ì‹­ìœ¡ìª½ì´ì—ìš”~"))
    print(remove_bracket("æèˆœè‡£ï¼ˆã‚¤ãƒ»ã‚¹ãƒ³ã‚·ãƒ³ï¼‰ãã®çŸ¥ã‚‰ã›ã‚’èã„ãŸäººã€…ã¯çš†ã€æèˆœè‡£ï¼ˆã‚¤ãƒ»ã‚¹ãƒ³ã‚·ãƒ³ï¼‰ã‚’æ°—ã‚’æ¯’ã«æ€((ã£ãŸã€‚."))
    print(remove_quotation("å½¼å¥³ã®ä»£è¡¨ä½œã€Œã‚ªãƒ«ãƒ©ãƒ³ãƒ‰ã€ã¯ã€ã‚¸ã‚§ãƒ³ãƒ€ãƒ¼ã¨æ€§ã®å•é¡Œã‚’æ­´å²çš„ãƒ»ç¤¾ä¼šçš„è„ˆçµ¡ã§è€ƒå¯Ÿã™ã‚‹ã€‚"))
    print(convert_enumeration("å½¼å¥³ã®ä»£è¡¨ä½œã€Œã‚ªãƒ«ãƒ©ãƒ³ãƒ‰ã€ã¯ã€ã‚¸ã‚§ãƒ³ãƒ€ãƒ¼ã¨æ€§ã®å•é¡Œã‚’æ­´å²çš„ãƒ»ç¤¾ä¼šçš„è„ˆçµ¡ã§è€ƒå¯Ÿã™ã‚‹ã€‚"))
    print(handle_for_correct_puncs("ê·¸ê²Œë§ì´ì•¼,ë„ˆ,ì •ë§  ì´ìƒí•´!"))